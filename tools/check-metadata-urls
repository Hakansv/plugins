#!/usr/bin/env python3

"""Check urls in ocpn-plugins.xml.

Digs for info-url and tarball-url in an ocpn-plugins.xml-ish file
and checks that all urls are accessible. Pure python3 without
external dependencies.

Diagnostics printouts goes to stdout, list of broken urls to stderr.

Usage:
    check-metadata-urls <path> [path]...


Arguments:
    path: File to check, defaults to ocpn-plugins.xml.
"""

import hashlib
import http.client as httplib
import xml.etree.ElementTree as ET
import socket
import sys
import tempfile
import urllib.request;

from http.client import HTTPException
from urllib.parse import urlparse


def get_status_code(urlstring):
    """ Retreive the status code of a website by requesting HEAD data
        (i. e., the headers) from host. If host cannot be reached
        or something else goes wrong, return None.

        See: http://stackoverflow.com/a/1140822/401554
    """
    url = urlparse(urlstring)
    headers = {
        'User-Agent':
            'Mozilla/5.0 (Macintosh; Intel Mac OS X x.y; rv:42.0)'
             + ' Gecko/20100101 Firefox/42.0'
    }
    try:
        if (url.scheme == 'https'):
            conn = httplib.HTTPSConnection(url.hostname)
        else:
            conn = httplib.HTTPConnection(url.hostname)
        conn.request("HEAD", url.path, None, headers)
        return conn.getresponse().status
    except (HTTPException, socket.gaierror):
        return None


def download_url(url, destdir):
    """ Download url, store in destdir. """
    basename = url.rsplit('/', 1)[1].rstrip()
    path = destdir + "/" + basename

    with urllib.request.urlopen(url) as _if:
        with open(path, "wb") as of:
            while True:
                block = _if.read(4096)
                if not block:
                    break
                of.write(block)
    return path


def check_plugin(tree):
    urls = [tree.find('./tarball-url').text.strip()]
    bad_urls = check_urls(urls)
    url_count = len(urls)
    metadata_cs = tree.find('./tarball-checksum')
    sha256_hash = hashlib.sha256()
    if len(bad_urls) == 0 and metadata_cs != None:
        metadata_cs = metadata_cs.text.strip()
        with tempfile.TemporaryDirectory() as td:
            # import pdb; pdb.set_trace()
            path = download_url(urls[0], td)
            with open(path, "rb") as f:
                for byte_block in iter(lambda: f.read(4096),b""):
                    sha256_hash.update(byte_block)
            if sha256_hash.hexdigest() != metadata_cs:
                bad_urls = [urls[0]]
                name = tree.find('./name').text.strip()
                print("")
                print(name + ": " + urls[0][urls[0].rfind("/") + 1:])
                print(name + (": metadata checksum: '%s'" % metadata_cs))
                print(name + (": computed checksum: '%s'" % sha256_hash.hexdigest()))

    urls = [tree.find('./info-url').text.strip()]
    bad_urls.extend(check_urls(urls))
    url_count += len(urls)
    return url_count, bad_urls


def check_urls(urls):
    """Check that each url in urls is accessible, return list
    of broken urls."""
    col = 0
    bad_urls = []
    for url in urls:
        if col > 72:
            print("")
            col = 0
        status = get_status_code(url.strip())
        dot = '.'
        if not status or status >= 400:
            dot = 'E'
            bad_urls.append(url + ": " + str(status))
        print(dot, end = '', flush = True)
        col +=1
    return bad_urls

def main():
    """Check urls in sys.argv[1], defaults to ocpn-plugins.xml."""
    check_cs = False
    if len(sys.argv) > 1 and sys.argv[1] == "-c":
        check_cs = True
        sys.argv = sys.argv[1:]
    if len(sys.argv) == 0:
        sys.argv.extend(["ocpn-plugins.xml"])
    bad_urls = []
    url_count = 0
    sys.argv = sys.argv[1:]
    for path in sys.argv:
        tree = ET.parse(path)
        if tree.getroot().tag == 'plugin':
            plugins = [tree]
        else:
            plugins = tree.findall('./plugin')
        for plugin in plugins:
            count, broken_urls = check_plugin(plugin)
            bad_urls.extend(broken_urls)
            url_count += count
    print("\n%d urls checked, %d errors" % (url_count, len(bad_urls)))
    for url in bad_urls:
        print(url, file = sys.stderr)
    sys.exit(0 if len(bad_urls) == 0 else 2)

if __name__ == '__main__':
    main()
